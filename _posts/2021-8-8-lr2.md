---
layout: post
title: Mitigating Unwanted Biases with Adversarial Learning
subtitle: Literature Review
date: 2021-8-8
author: JIM
header-img: img/post-bg-coffee.jpeg
catalog: true
mathjax: true
tags:
  - Algorithm
  - Gender Bias
  - Literature Review
  - Word Embeddings
  - NLP
  - Adversarial Learning
---

## Mitigating Unwanted Biases with Adversarial Learning

[PDF](https://arxiv.org/abs/1801.07593)

Published in 2018

Cited by 438

### Intro

This paper proposed an **adversarial architecture** of networks that can be used to, on one hand, maximize the predicator's ability to predict, on the other hand, minimize the adversary's ability to predict some protected attributes. The architecture is flexible and can be applied in a wide range of gradient-based learning models (**downstream tasks of word embeddings**). For example, training a model to perform the analogy task that is less prone to gender bias.

### Formulation
Consider supervised learning tasks that predict an output variable $Y$ given an input $X$, while remaining unbiased with respect to some variable $Z$.

### Architecture
![](https://i.imgur.com/idGuxMe.png)
We begin with a model, which we call the predictor, trained to accomplish the task of predicting $Y$ given $X$. As in Figure 1 , we assume that the model is trained by attempting to modify weights $W$ to minimize some loss $L_{P}(\hat{y}, y)$, using a gradient-based method such as stochastic gradient descent. The output layer of the predictor is then used as an input to another network called the adversary which attempts to predict $Z$. This is part of the network corresponds to the discriminator in a typical GAN (Goodfellow et al. 2014). We will suppose the adversary has loss term $L_{A}(\hat{z}, z)$ and weights $U$. Depending on the definition of fairness being achieved, the adversary may have other inputs.

### **How to update predicator $W$**
The predicator weights $W$ will be updated by
$$\nabla_{W} L_{P}-\operatorname{proj}_{\nabla_{W} L_{A}} \nabla_{W} L_{P}-\alpha \nabla_{W} L_{A}$$
where $\operatorname{proj}_{u}v$ is the projection length of $v$ onto direction of $u$, i.e. $\operatorname{proj}_{u}v=u^\top v/\|u\|$.

The first term $\nabla_{W} L_{P}$ is simply to optimize the prediction objective.
The third term $-\alpha \nabla_{W} L_{A}$ is to increase adversary loss.
The second term $-\operatorname{proj}_{\nabla_{W} L_{A}} \nabla_{W} L_{P}$ is to prevent $W$ from helping adversary decrease its loss.

### Experiments
1. Gender-fair analogy task
![](https://i.imgur.com/Fapke6y.png)
2. Gender-fair income prediction task
![](https://i.imgur.com/AFIZdUh.png)
*Why to use FPR and FNR? TODO: investigate the metrics.*
